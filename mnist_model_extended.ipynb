{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "b3QokrtPMFMA",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## MNIST model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Let's see how we can train a simple model on the mnist dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### What we will do:\n",
    "- we will get the data (mnist dataset)\n",
    "- we will define the target (recognize the digits)\n",
    "- we will build the model\n",
    "- we will train the model\n",
    "- we will verify that the model can classify randomly selected samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "We will start by importing some usefull tools\n",
    "\n",
    "- **keras** is for building and training the neral network\n",
    "- **numpy** is for handling numerical data\n",
    "- **matplotlib**, **IPython** and **tabulate** are tools for printing and plotting (e.g. tables or images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "S4W_uQ3ryGIx",
    "outputId": "2178fcd1-a9ff-4408-b1ac-b7fd34acd93b",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "import keras\n",
    "from keras.datasets import *\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import *\n",
    "from keras.activations import softmax, relu\n",
    "from keras.losses import categorical_crossentropy\n",
    "from keras.optimizers import *\n",
    "from keras.utils import to_categorical\n",
    "import keras.backend as K\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import Image\n",
    "from tabulate import tabulate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## The data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Rt1qyRCZMhZA",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Now we need the data\n",
    "\n",
    "The data are small (28x28 pixels) gray scale images of hand-written digits."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "This is what the data look like\n",
    "![mnist_sample](https://www.researchgate.net/profile/Steven_Young11/publication/306056875/figure/fig1/AS:393921575309346@1470929630835/Example-images-from-the-MNIST-dataset.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Let's load the data..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "(X_train, Y_train), (X_val, Y_val) = mnist.load_data()\n",
    "labels_names = 'zero', 'one', 'two', 'three', 'four', 'five', 'six', 'seven', 'eight', 'nine'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "B1__zwt1NYmH",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "All the data that we use when training DL models are actually n-dimensional ***arrays*** with **numerical** values\n",
    "\n",
    "No matter if it was originally a video, an image, a voice record or a text, in the end **everything is transformed to arrays**\n",
    "\n",
    "The **shape** (as well the **range** of the values) of the array is important since the models are built to be able to handle specific kind of arrays regarding the shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "data_info = [(name, d.shape, d.min(), d.max()) \n",
    "             for name, d in zip(('X_train', 'Y_train', 'X_val', 'Y_val'),\n",
    "                                (X_train, Y_train, X_val, Y_val))]\n",
    "\n",
    "print(tabulate(data_info, headers=['name', 'shape', 'minimum', 'maximum']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Originally the images' pixels have values in [0, 255]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "However that big values are not easy to be handled by the networks\n",
    "\n",
    "Thus we usually change the input values to something more \"model friendly\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "This is called data preprocessing\n",
    "\n",
    "In our case the preprocessing is just a mapping of the values from [0, 255] to [0, 1]\n",
    "\n",
    "by dividing the array's values by 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "X_train, X_val = X_train / 255, X_val / 255"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "But what do these \"images\" look like?\n",
    "\n",
    "Let's see what is inside the first \"image\" of our training dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 493
    },
    "colab_type": "code",
    "id": "vmTQoKzXs5x5",
    "outputId": "62c94ea2-8c44-4985-ef39-cbd94044df96",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "index = 0\n",
    "img = X_train[index]\n",
    "for r in np.round(img, 2):\n",
    "  print(*r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 347
    },
    "colab_type": "code",
    "id": "jlSna3Fkr3Tw",
    "outputId": "e1c11a85-8c75-4057-deb2-8e49f54a4cd6",
    "scrolled": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "plt.imshow(X_train[index], cmap='gray')\n",
    "print('label:', Y_train[index])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "b4mPcD5QOLCn",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "In the next cell we define some functions for getting random images from the dataset and plotting them"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Don't pay too much attention to them for the moment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pqY001s4AdpE",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "r, c = 3, 3\n",
    "\n",
    "def get_random_imgs_labels(X_set, Y_set, n_imgs):\n",
    "  inds = np.random.randint(0, len(X_set), n_imgs)\n",
    "  images, labels = X_set[inds], Y_set[inds]\n",
    "  return images, labels\n",
    "\n",
    "\n",
    "def plot_images(images, labels, labels_names, preds=None):\n",
    "  labels = labels.flatten()\n",
    "  fig, axs = plt.subplots(r, c)\n",
    "  cnt = 0\n",
    "  for i in range(r):\n",
    "    for j in range(c):\n",
    "      axs[i, j].imshow(images[cnt], cmap='gray')\n",
    "      axs[i, j].axis('off')\n",
    "      title = labels_names[labels[cnt]] if preds is None else '%s/%s' % (labels_names[labels[cnt]], labels_names[preds[cnt]])\n",
    "      axs[i, j].set_title(title, fontsize=12)\n",
    "      cnt += 1\n",
    "  plt.show()\n",
    "  \n",
    "    \n",
    "def get_subset(xs, ys, l):\n",
    "  n_xs, n_ys = [], []\n",
    "  for x, y in zip(xs, ys):\n",
    "    if y in l:\n",
    "      n_xs.append(x)\n",
    "      n_ys.append(y)\n",
    "  n_xs, n_ys = np.array(n_xs), np.array(n_ys)\n",
    "  return n_xs, n_ys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NnO5cfBYOahx",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Let's plot some of the images together with their labels to see what the look like"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 365
    },
    "colab_type": "code",
    "id": "qEBEFVxE5UPl",
    "outputId": "eb109eec-6b77-4f5f-bd57-5c14adef3157",
    "scrolled": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "images, labels = get_random_imgs_labels(X_train, Y_train, r*c)\n",
    "plot_images(images, labels, labels_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## The model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9xmto0cDOkLx",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Now we have to build the model that we will use to predict the label of a given image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 238
    },
    "colab_type": "code",
    "id": "7ZB0aex57iWJ",
    "outputId": "c4c2d719-7c78-408b-989d-843f6773828d",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "K.clear_session()\n",
    "\n",
    "model = Sequential([\n",
    "    Flatten(input_shape=(28, 28)),\n",
    "    Dense(32, activation='relu'),\n",
    "    Dense(10, activation='softmax')])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Our model has a *Flatten* layer and 2 *Fully connected* layers\n",
    "![Flatten layer](https://sds-platform-private.s3-us-east-2.amazonaws.com/uploads/73_blog_image_1.png)\n",
    "![Fully connected model](https://sds-platform-private.s3-us-east-2.amazonaws.com/uploads/74_blog_image_1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The model's output is a list of 10 numbers; one for each category of our dataset\n",
    "\n",
    "By using ***softmax*** as activation of the last layer we constrain these numbers to:\n",
    "- be between 0 and 1\n",
    "- have their sum equal to 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "This way we can interpret them as probabilities for the categories"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "We use the label with the highest probability as the predicted one"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "For example if the output is:\n",
    "\n",
    "```0.002, 0.013, 0.017, 0.006, 0.027, 0.109, 0.024, 0.789, 0.002, 0.011```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "The predicted label will be: ***seven***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Q_-S7nT9POea",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Before we start the training of the model we need to define\n",
    "- the target\n",
    "- the way to achieve it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "In our case we want \n",
    "- the model's outputs\n",
    "- the given probabilities for each label\n",
    "\n",
    "to be as close as possible to each other"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Since in each case we have only one correct label, we want ideally the model to return probability 1 for the correct label and 0 for the rest ones"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "To ahcieve this we use a ***loss function***\n",
    "\n",
    "In our case the loss function will be the *categorical crossentropy*:\n",
    "\n",
    "$$H(p,q) = - \\sum_x p(x) \\log(q(x))$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "y_true = 0, 0, 0, 0, 0, 0, 0, 1, 0, 0\n",
    "y_pred = 0.002, 0.013, 0.017, 0.006, 0.027, 0.109, 0.024, 0.789, 0.002, 0.011\n",
    "\n",
    "loss = -sum([p*np.log(q) for p, q in zip(y_true, y_pred)])\n",
    "print('loss based on formula:       ', np.round(loss, 5))\n",
    "\n",
    "loss = keras.losses.categorical_crossentropy(K.variable(y_true), K.variable(y_pred))\n",
    "print('loss based on keras function:', np.round(K.eval(loss), 5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "We can make some changes to the numbers and obtain the resulted loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "We also need to define a method based upon the model will try to **minimize** the loss function.\n",
    "\n",
    "The method (also called **optimizer** since it optimize the model's parameters) that we will use is ***Adam***\n",
    "\n",
    "We don't need to get into too much details for this one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ahRvgdb0PTCM",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "model.compile(optimizer=Adam(), loss=categorical_crossentropy, metrics=['acc'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7cL-KUfrP6Hj",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Now the model has been randomly initialized which means that the outputs will be mostly wrong"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Let's see some examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 382
    },
    "colab_type": "code",
    "id": "utMajqL0_qET",
    "outputId": "62c44ec1-9e27-4ae2-b6db-67a684bfaaa6",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "images, labels = get_random_imgs_labels(X_val, Y_val, r*c)\n",
    "predictions = model.predict_on_batch(images)\n",
    "predictions = np.argmax(predictions, -1)\n",
    "\n",
    "print('correct: %d out of %d' % (np.sum(labels == predictions), len(images)))\n",
    "plot_images(images, labels, labels_names, predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SgIiPVedQa3A",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Now let's train the model for some epochs and see if we can imporve the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 374
    },
    "colab_type": "code",
    "id": "FN_bprgkFw0T",
    "outputId": "8bf194c3-8c63-43de-aa05-3260e4b86c88",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "history = model.fit(X_train, to_categorical(Y_train),\n",
    "                    validation_data=(X_val, to_categorical(Y_val)),\n",
    "                    batch_size=64, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 382
    },
    "colab_type": "code",
    "id": "utMajqL0_qET",
    "outputId": "62c44ec1-9e27-4ae2-b6db-67a684bfaaa6",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "images, labels = get_random_imgs_labels(X_val, Y_val, 9)\n",
    "predictions = model.predict_on_batch(images)\n",
    "predictions = np.argmax(predictions, 1)\n",
    "\n",
    "print('correct: %d out of %d' % (np.sum(labels == predictions), len(labels)))\n",
    "plot_images(images, labels, labels_names, predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tMkRFad_Q3mC",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "That was it!\n",
    "\n",
    "We trained a model to classify images of handwritten digits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### To summarize:\n",
    "- we got the data (mnist dataset)\n",
    "- we defined the target (recognize the digits)\n",
    "- we built the model\n",
    "- we trained the model\n",
    "- we verified that the model can classify randomly selected samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## The end\n",
    "\n",
    "### of the simple MNIST classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Data matters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "It is commonly said that in DL you need **Big Data**\n",
    "\n",
    "But how big must your data be?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "In the previous case we had 60,000 training examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "and we achieved ~97% accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "which is quite good given the simplicity of the model and the training process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "But what happens if we have **fewer** data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "LfFXSfKDurzk",
    "outputId": "ac8585f1-f6ba-44ae-87f1-fc71df27c720",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "short_sample = 50\n",
    "\n",
    "X_train_short = X_train[:short_sample]\n",
    "Y_train_short = Y_train[:short_sample]\n",
    "\n",
    "X_train_short.shape, Y_train_short.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 3434
    },
    "colab_type": "code",
    "id": "r1z-PstQvGIu",
    "outputId": "881160d1-be38-4436-c1a8-47f4300074d2",
    "scrolled": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "K.clear_session()\n",
    "\n",
    "model = Sequential([\n",
    "    Flatten(input_shape=(28, 28)),\n",
    "    Dense(32, activation='relu'),\n",
    "    Dense(10, activation='softmax')\n",
    "    ])\n",
    "\n",
    "model.compile(optimizer=Adam(), loss=categorical_crossentropy, metrics=['acc'])\n",
    "\n",
    "history = model.fit(X_train_short, to_categorical(Y_train_short),\n",
    "                    validation_data=(X_val, to_categorical(Y_val)),\n",
    "                    batch_size=50, epochs=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "images, labels = get_random_imgs_labels(X_val, Y_val, 9)\n",
    "predictions = model.predict_on_batch(images)\n",
    "predictions = np.argmax(predictions, 1)\n",
    "\n",
    "print('correct: %d out of %d' % (np.sum(labels == predictions), len(labels)))\n",
    "plot_images(images, labels, labels_names, predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "In this case we see that the accuracy of the model on the training data is very high\n",
    "\n",
    "But on the validation data in is significantly lower"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "This means that the model has ***memorized*** the training data\n",
    "\n",
    "But it cannot generalize to new/unseen images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "This is called ***Overfitting***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "There are some ways to reduce overfitting but it is out of the scope of this example\n",
    "\n",
    "And if the data are too few, there is not much to be done"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "However, if the problem was simpler the same amount of data might be enough"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Let's say for example that we want to distinguish only between 0s and 1s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "X_train_01, Y_train_01 = get_subset(X_train, Y_train, (0, 1))\n",
    "X_val_01, Y_val_01 = get_subset(X_val, Y_val, (0, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Let's plot some of the images together with their labels to see what the look like"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 365
    },
    "colab_type": "code",
    "id": "8nHZZDH1vNG2",
    "outputId": "355474e1-0301-458a-b951-595cb192ff0c",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "images, labels = get_random_imgs_labels(X_train_01, Y_train_01, r*c)\n",
    "plot_images(images, labels, labels_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "And now let's keep only few of the training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "short_sample = 50\n",
    "\n",
    "X_train_01_short = X_train_01[:short_sample]\n",
    "Y_train_01_short = Y_train_01[:short_sample]\n",
    "\n",
    "X_train_01_short.shape, Y_train_01_short.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Now we will train a new model on the new data\n",
    "\n",
    "Pay attention at the number of units at the output layer\n",
    "\n",
    "Since we have only 2 possible labels, we have 2 output units"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 3434
    },
    "colab_type": "code",
    "id": "IsjRH00AxCQp",
    "outputId": "e073384d-2374-49a2-ca15-60a83a1a9ca4",
    "scrolled": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "K.clear_session()\n",
    "\n",
    "model = Sequential([\n",
    "    Flatten(input_shape=(28, 28)),\n",
    "    Dense(32, activation='relu'),\n",
    "    Dense(2, activation='softmax')\n",
    "    ])\n",
    "\n",
    "model.compile(optimizer=Adam(), loss=categorical_crossentropy, metrics=['acc'])\n",
    "\n",
    "history = model.fit(X_train_01_short, to_categorical(Y_train_01_short),\n",
    "                    validation_data=(X_val_01, to_categorical(Y_val_01)),\n",
    "                    batch_size=50, epochs=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The results are significantly better\n",
    "\n",
    "Let's plot some examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 382
    },
    "colab_type": "code",
    "id": "0ynb2NC5xCN5",
    "outputId": "6e99feb1-e25d-4c05-e8e0-ddd2610f4c7c",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "images, labels = get_random_imgs_labels(X_val_01, Y_val_01, r*c)\n",
    "predictions = model.predict_on_batch(images)\n",
    "predictions = np.argmax(predictions, 1)\n",
    "\n",
    "print('correct: %d out of %d' % (np.sum(labels == predictions), len(labels)))\n",
    "plot_images(images, labels, labels_names, predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "This means that:\n",
    "- even with the **same type** of data\n",
    "- for a **different task**\n",
    "- **different amount** of data might be needed\n",
    "- for the **same** level of accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Data matters\n",
    "\n",
    "What about the model?\n",
    "\n",
    "It is also said that deeper models have better performance\n",
    "\n",
    "This is why it is called **Deep** learning after all"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "In the previous example the model had already a good performance\n",
    "\n",
    "But let's try now the same model on a more difficult task\n",
    "\n",
    "We will use CIFAR 10, a dataset of small (32x32 pixels) colored images of different categories"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "### CIFAR 10 samples\n",
    "![Flatten layer](https://storage.googleapis.com/kaggle-competitions/kaggle/3649/media/cifar-10.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "(X_train, Y_train), (X_val, Y_val) = cifar10.load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Let's see what our data look like this time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "data_info = [(name, d.shape, d.min(), d.max()) \n",
    "             for name, d in zip(('X_train', 'Y_train', 'X_val', 'Y_val'),\n",
    "                                (X_train, Y_train, X_val, Y_val))]\n",
    "\n",
    "print(tabulate(data_info, headers=['name', 'shape', 'minimum', 'maximum']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "and let's normalize the data (in range [0, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "X_train, X_val = X_train / 255, X_val / 255\n",
    "labels_names = 'airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "These are some samples of our new dataset among with their categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "images, labels = get_random_imgs_labels(X_train, Y_train, r*c)\n",
    "plot_images(images, labels, labels_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Let's train the original model on the new dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 50000 samples, validate on 10000 samples\n",
      "Epoch 1/30\n",
      "50000/50000 [==============================] - 2s 39us/step - loss: 2.2064 - acc: 0.1645 - val_loss: 2.1543 - val_acc: 0.1830\n",
      "Epoch 2/30\n",
      "50000/50000 [==============================] - 2s 36us/step - loss: 2.1316 - acc: 0.1788 - val_loss: 2.1096 - val_acc: 0.1823\n",
      "Epoch 3/30\n",
      "50000/50000 [==============================] - 2s 36us/step - loss: 2.1038 - acc: 0.1777 - val_loss: 2.0895 - val_acc: 0.1838\n",
      "Epoch 4/30\n",
      "50000/50000 [==============================] - 2s 37us/step - loss: 2.0881 - acc: 0.1797 - val_loss: 2.0799 - val_acc: 0.1824\n",
      "Epoch 5/30\n",
      "50000/50000 [==============================] - 2s 37us/step - loss: 2.0779 - acc: 0.1820 - val_loss: 2.0795 - val_acc: 0.1847\n",
      "Epoch 6/30\n",
      "50000/50000 [==============================] - 2s 37us/step - loss: 2.0733 - acc: 0.1812 - val_loss: 2.0660 - val_acc: 0.1857\n",
      "Epoch 7/30\n",
      "50000/50000 [==============================] - 2s 39us/step - loss: 2.0699 - acc: 0.1831 - val_loss: 2.0755 - val_acc: 0.1809\n",
      "Epoch 8/30\n",
      "50000/50000 [==============================] - 2s 37us/step - loss: 2.0675 - acc: 0.1837 - val_loss: 2.0620 - val_acc: 0.1837\n",
      "Epoch 9/30\n",
      "50000/50000 [==============================] - 2s 37us/step - loss: 2.0633 - acc: 0.1813 - val_loss: 2.0601 - val_acc: 0.1879\n",
      "Epoch 10/30\n",
      "50000/50000 [==============================] - 2s 37us/step - loss: 2.0625 - acc: 0.1846 - val_loss: 2.0567 - val_acc: 0.1859\n",
      "Epoch 11/30\n",
      "50000/50000 [==============================] - 2s 38us/step - loss: 2.0589 - acc: 0.1840 - val_loss: 2.0732 - val_acc: 0.1801\n",
      "Epoch 12/30\n",
      "50000/50000 [==============================] - 2s 39us/step - loss: 2.0579 - acc: 0.1849 - val_loss: 2.0555 - val_acc: 0.1837\n",
      "Epoch 13/30\n",
      "50000/50000 [==============================] - 2s 39us/step - loss: 2.0568 - acc: 0.1855 - val_loss: 2.0694 - val_acc: 0.1791\n",
      "Epoch 14/30\n",
      "50000/50000 [==============================] - 2s 38us/step - loss: 2.0569 - acc: 0.1842 - val_loss: 2.0510 - val_acc: 0.1870\n",
      "Epoch 15/30\n",
      "50000/50000 [==============================] - 2s 39us/step - loss: 2.0558 - acc: 0.1858 - val_loss: 2.0705 - val_acc: 0.1899\n",
      "Epoch 16/30\n",
      "50000/50000 [==============================] - 2s 38us/step - loss: 2.0533 - acc: 0.1849 - val_loss: 2.0579 - val_acc: 0.1907\n",
      "Epoch 17/30\n",
      "50000/50000 [==============================] - 2s 38us/step - loss: 2.0501 - acc: 0.1863 - val_loss: 2.0472 - val_acc: 0.1905\n",
      "Epoch 18/30\n",
      "50000/50000 [==============================] - 2s 38us/step - loss: 2.0513 - acc: 0.1881 - val_loss: 2.0533 - val_acc: 0.1889\n",
      "Epoch 19/30\n",
      "50000/50000 [==============================] - 2s 38us/step - loss: 2.0490 - acc: 0.1864 - val_loss: 2.0655 - val_acc: 0.1848\n",
      "Epoch 20/30\n",
      "50000/50000 [==============================] - 2s 38us/step - loss: 2.0479 - acc: 0.1887 - val_loss: 2.0451 - val_acc: 0.1890\n",
      "Epoch 21/30\n",
      "50000/50000 [==============================] - 2s 38us/step - loss: 2.0471 - acc: 0.1886 - val_loss: 2.0638 - val_acc: 0.1867\n",
      "Epoch 22/30\n",
      "50000/50000 [==============================] - 2s 38us/step - loss: 2.0462 - acc: 0.1878 - val_loss: 2.0648 - val_acc: 0.1815\n",
      "Epoch 23/30\n",
      "50000/50000 [==============================] - 2s 38us/step - loss: 2.0443 - acc: 0.1881 - val_loss: 2.0419 - val_acc: 0.1894\n",
      "Epoch 24/30\n",
      "50000/50000 [==============================] - 2s 38us/step - loss: 2.0438 - acc: 0.1890 - val_loss: 2.0806 - val_acc: 0.1809\n",
      "Epoch 25/30\n",
      "50000/50000 [==============================] - 2s 38us/step - loss: 2.0425 - acc: 0.1868 - val_loss: 2.0419 - val_acc: 0.1838\n",
      "Epoch 26/30\n",
      "50000/50000 [==============================] - 2s 38us/step - loss: 2.0423 - acc: 0.1897 - val_loss: 2.0403 - val_acc: 0.1789\n",
      "Epoch 27/30\n",
      "50000/50000 [==============================] - 2s 38us/step - loss: 2.0387 - acc: 0.1898 - val_loss: 2.0476 - val_acc: 0.1904\n",
      "Epoch 28/30\n",
      "50000/50000 [==============================] - 2s 38us/step - loss: 2.0397 - acc: 0.1911 - val_loss: 2.0396 - val_acc: 0.1948\n",
      "Epoch 29/30\n",
      "50000/50000 [==============================] - 2s 38us/step - loss: 2.0382 - acc: 0.1897 - val_loss: 2.0425 - val_acc: 0.1835\n",
      "Epoch 30/30\n",
      "50000/50000 [==============================] - 2s 38us/step - loss: 2.0394 - acc: 0.1892 - val_loss: 2.0382 - val_acc: 0.1958\n"
     ]
    }
   ],
   "source": [
    "K.clear_session()\n",
    "\n",
    "model = Sequential([\n",
    "    Flatten(input_shape=(32, 32, 3)),\n",
    "    Dense(32, activation='relu'),\n",
    "    Dense(10, activation='softmax')\n",
    "    ])\n",
    "\n",
    "model.compile(optimizer=Adam(), loss=categorical_crossentropy, metrics=['acc'])\n",
    "\n",
    "history = model.fit(X_train, to_categorical(Y_train),\n",
    "                    validation_data=(X_val, to_categorical(Y_val)),\n",
    "                    batch_size=128, epochs=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The performance is much worse this time\n",
    "\n",
    "Let's see if adding more layers will make any difference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 50000 samples, validate on 10000 samples\n",
      "Epoch 1/30\n",
      "50000/50000 [==============================] - 3s 50us/step - loss: 1.8741 - acc: 0.3162 - val_loss: 1.7050 - val_acc: 0.3856\n",
      "Epoch 2/30\n",
      "50000/50000 [==============================] - 2s 46us/step - loss: 1.7049 - acc: 0.3852 - val_loss: 1.6591 - val_acc: 0.3975\n",
      "Epoch 3/30\n",
      "50000/50000 [==============================] - 2s 44us/step - loss: 1.6415 - acc: 0.4082 - val_loss: 1.6294 - val_acc: 0.4143\n",
      "Epoch 4/30\n",
      "50000/50000 [==============================] - 2s 44us/step - loss: 1.5833 - acc: 0.4296 - val_loss: 1.5722 - val_acc: 0.4330\n",
      "Epoch 5/30\n",
      "50000/50000 [==============================] - 2s 46us/step - loss: 1.5526 - acc: 0.4396 - val_loss: 1.5309 - val_acc: 0.4539\n",
      "Epoch 6/30\n",
      "50000/50000 [==============================] - 2s 43us/step - loss: 1.5114 - acc: 0.4580 - val_loss: 1.5156 - val_acc: 0.4568\n",
      "Epoch 7/30\n",
      "50000/50000 [==============================] - 2s 46us/step - loss: 1.4937 - acc: 0.4633 - val_loss: 1.5144 - val_acc: 0.4555\n",
      "Epoch 8/30\n",
      "50000/50000 [==============================] - 2s 45us/step - loss: 1.4626 - acc: 0.4743 - val_loss: 1.5317 - val_acc: 0.4594\n",
      "Epoch 9/30\n",
      "50000/50000 [==============================] - 2s 46us/step - loss: 1.4486 - acc: 0.4808 - val_loss: 1.5106 - val_acc: 0.4646\n",
      "Epoch 10/30\n",
      "50000/50000 [==============================] - 2s 45us/step - loss: 1.4220 - acc: 0.4874 - val_loss: 1.4596 - val_acc: 0.4813\n",
      "Epoch 11/30\n",
      "50000/50000 [==============================] - 2s 46us/step - loss: 1.4059 - acc: 0.4939 - val_loss: 1.4610 - val_acc: 0.4765\n",
      "Epoch 12/30\n",
      "50000/50000 [==============================] - 2s 46us/step - loss: 1.3962 - acc: 0.4975 - val_loss: 1.5078 - val_acc: 0.4629\n",
      "Epoch 13/30\n",
      "50000/50000 [==============================] - 2s 46us/step - loss: 1.3828 - acc: 0.5052 - val_loss: 1.4597 - val_acc: 0.4803\n",
      "Epoch 14/30\n",
      "50000/50000 [==============================] - 2s 45us/step - loss: 1.3638 - acc: 0.5126 - val_loss: 1.4359 - val_acc: 0.4938\n",
      "Epoch 15/30\n",
      "50000/50000 [==============================] - 2s 45us/step - loss: 1.3549 - acc: 0.5160 - val_loss: 1.4483 - val_acc: 0.4890\n",
      "Epoch 16/30\n",
      "50000/50000 [==============================] - 2s 46us/step - loss: 1.3384 - acc: 0.5210 - val_loss: 1.4749 - val_acc: 0.4844\n",
      "Epoch 17/30\n",
      "50000/50000 [==============================] - 2s 46us/step - loss: 1.3242 - acc: 0.5239 - val_loss: 1.4435 - val_acc: 0.4914\n",
      "Epoch 18/30\n",
      "50000/50000 [==============================] - 2s 46us/step - loss: 1.3164 - acc: 0.5262 - val_loss: 1.4494 - val_acc: 0.4855\n",
      "Epoch 19/30\n",
      "50000/50000 [==============================] - 2s 46us/step - loss: 1.3013 - acc: 0.5321 - val_loss: 1.4609 - val_acc: 0.4837\n",
      "Epoch 20/30\n",
      "50000/50000 [==============================] - 2s 47us/step - loss: 1.2961 - acc: 0.5353 - val_loss: 1.4767 - val_acc: 0.4816\n",
      "Epoch 21/30\n",
      "50000/50000 [==============================] - 3s 50us/step - loss: 1.2837 - acc: 0.5393 - val_loss: 1.4458 - val_acc: 0.4902\n",
      "Epoch 22/30\n",
      "50000/50000 [==============================] - 3s 53us/step - loss: 1.2685 - acc: 0.5434 - val_loss: 1.4522 - val_acc: 0.4887\n",
      "Epoch 23/30\n",
      "50000/50000 [==============================] - 3s 55us/step - loss: 1.2562 - acc: 0.5485 - val_loss: 1.4636 - val_acc: 0.4830\n",
      "Epoch 24/30\n",
      "50000/50000 [==============================] - 2s 49us/step - loss: 1.2441 - acc: 0.5516 - val_loss: 1.4445 - val_acc: 0.4944\n",
      "Epoch 25/30\n",
      "50000/50000 [==============================] - 2s 46us/step - loss: 1.2350 - acc: 0.5547 - val_loss: 1.4756 - val_acc: 0.4845\n",
      "Epoch 26/30\n",
      "50000/50000 [==============================] - 2s 46us/step - loss: 1.2261 - acc: 0.5557 - val_loss: 1.5146 - val_acc: 0.4781\n",
      "Epoch 27/30\n",
      "50000/50000 [==============================] - 2s 47us/step - loss: 1.2146 - acc: 0.5603 - val_loss: 1.4778 - val_acc: 0.4855\n",
      "Epoch 28/30\n",
      "50000/50000 [==============================] - 2s 47us/step - loss: 1.2070 - acc: 0.5640 - val_loss: 1.4706 - val_acc: 0.4910\n",
      "Epoch 29/30\n",
      "50000/50000 [==============================] - 2s 49us/step - loss: 1.1955 - acc: 0.5683 - val_loss: 1.5065 - val_acc: 0.4868\n",
      "Epoch 30/30\n",
      "50000/50000 [==============================] - 2s 48us/step - loss: 1.1869 - acc: 0.5702 - val_loss: 1.4861 - val_acc: 0.4846\n"
     ]
    }
   ],
   "source": [
    "K.clear_session()\n",
    "\n",
    "model = Sequential([\n",
    "    Flatten(input_shape=(32, 32, 3)),\n",
    "    Dense(32, activation='relu'),\n",
    "    Dense(64, activation='relu'),  # New layer\n",
    "    Dense(128, activation='relu'),  # New layer\n",
    "    Dense(256, activation='relu'),  # New layer\n",
    "    Dense(10, activation='softmax')\n",
    "    ])\n",
    "\n",
    "model.compile(optimizer=Adam(), loss=categorical_crossentropy, metrics=['acc'])\n",
    "\n",
    "history = model.fit(X_train, to_categorical(Y_train),\n",
    "                    validation_data=(X_val, to_categorical(Y_val)),\n",
    "                    batch_size=128, epochs=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Making the model deeper improved its performance a bit\n",
    "\n",
    "But can we do any better?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Now let's try a different type of model\n",
    "\n",
    "This type is called ***Convolutional*** and uses a specific type of layer which is very popular for image related tasks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The idea is that at every layer of the model there are some filters that learn specific patterns or characteristics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "The first layers learn low level (simple) patterns. The deeper layer learn to recognize more complex patterns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Here is an example of filters trained to recognize human faces\n",
    "\n",
    "See how the complexity of the patterns increases as we go deeper\n",
    "\n",
    "![convolutional kernels](https://devblogs.nvidia.com/wp-content/uploads/2015/11/hierarchical_features.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 50000 samples, validate on 10000 samples\n",
      "Epoch 1/30\n",
      "50000/50000 [==============================] - 32s 641us/step - loss: 1.5886 - acc: 0.4405 - val_loss: 1.2987 - val_acc: 0.5355\n",
      "Epoch 2/30\n",
      "50000/50000 [==============================] - 24s 489us/step - loss: 1.2117 - acc: 0.5762 - val_loss: 1.2323 - val_acc: 0.5666\n",
      "Epoch 3/30\n",
      "50000/50000 [==============================] - 25s 500us/step - loss: 1.0767 - acc: 0.6239 - val_loss: 1.1703 - val_acc: 0.5891\n",
      "Epoch 4/30\n",
      "50000/50000 [==============================] - 25s 505us/step - loss: 0.9769 - acc: 0.6594 - val_loss: 1.1828 - val_acc: 0.5875\n",
      "Epoch 5/30\n",
      "50000/50000 [==============================] - 25s 504us/step - loss: 0.8795 - acc: 0.6943 - val_loss: 1.1166 - val_acc: 0.6127\n",
      "Epoch 6/30\n",
      "50000/50000 [==============================] - 24s 490us/step - loss: 0.7911 - acc: 0.7249 - val_loss: 1.1513 - val_acc: 0.6161\n",
      "Epoch 7/30\n",
      "50000/50000 [==============================] - 24s 489us/step - loss: 0.7044 - acc: 0.7563 - val_loss: 1.1638 - val_acc: 0.6121\n",
      "Epoch 8/30\n",
      "50000/50000 [==============================] - 24s 489us/step - loss: 0.6249 - acc: 0.7844 - val_loss: 1.1985 - val_acc: 0.6146\n",
      "Epoch 9/30\n",
      "50000/50000 [==============================] - 24s 489us/step - loss: 0.5435 - acc: 0.8132 - val_loss: 1.2676 - val_acc: 0.6030\n",
      "Epoch 10/30\n",
      "50000/50000 [==============================] - 24s 488us/step - loss: 0.4651 - acc: 0.8438 - val_loss: 1.2859 - val_acc: 0.6108\n",
      "Epoch 11/30\n",
      "50000/50000 [==============================] - 24s 489us/step - loss: 0.3909 - acc: 0.8728 - val_loss: 1.3981 - val_acc: 0.5998\n",
      "Epoch 12/30\n",
      "50000/50000 [==============================] - 24s 489us/step - loss: 0.3133 - acc: 0.9013 - val_loss: 1.4909 - val_acc: 0.6074\n",
      "Epoch 13/30\n",
      "50000/50000 [==============================] - 24s 489us/step - loss: 0.2608 - acc: 0.9208 - val_loss: 1.5908 - val_acc: 0.6021\n",
      "Epoch 14/30\n",
      "50000/50000 [==============================] - 24s 488us/step - loss: 0.2073 - acc: 0.9390 - val_loss: 1.6656 - val_acc: 0.6059\n",
      "Epoch 15/30\n",
      "50000/50000 [==============================] - 24s 488us/step - loss: 0.1635 - acc: 0.9547 - val_loss: 1.8041 - val_acc: 0.5994\n",
      "Epoch 16/30\n",
      "50000/50000 [==============================] - 24s 488us/step - loss: 0.1346 - acc: 0.9636 - val_loss: 1.9002 - val_acc: 0.5927\n",
      "Epoch 17/30\n",
      "50000/50000 [==============================] - 24s 488us/step - loss: 0.1110 - acc: 0.9705 - val_loss: 2.0340 - val_acc: 0.5914\n",
      "Epoch 18/30\n",
      "50000/50000 [==============================] - 24s 490us/step - loss: 0.0871 - acc: 0.9785 - val_loss: 2.1624 - val_acc: 0.5906\n",
      "Epoch 19/30\n",
      "50000/50000 [==============================] - 24s 489us/step - loss: 0.0667 - acc: 0.9858 - val_loss: 2.2045 - val_acc: 0.6014\n",
      "Epoch 20/30\n",
      "50000/50000 [==============================] - 24s 489us/step - loss: 0.0562 - acc: 0.9879 - val_loss: 2.3693 - val_acc: 0.5926\n",
      "Epoch 21/30\n",
      "50000/50000 [==============================] - 24s 489us/step - loss: 0.0513 - acc: 0.9881 - val_loss: 2.3959 - val_acc: 0.5884\n",
      "Epoch 22/30\n",
      "50000/50000 [==============================] - 24s 489us/step - loss: 0.0887 - acc: 0.9720 - val_loss: 2.4530 - val_acc: 0.5936\n",
      "Epoch 23/30\n",
      "50000/50000 [==============================] - 24s 490us/step - loss: 0.0695 - acc: 0.9794 - val_loss: 2.5083 - val_acc: 0.5845\n",
      "Epoch 24/30\n",
      "50000/50000 [==============================] - 25s 490us/step - loss: 0.0487 - acc: 0.9875 - val_loss: 2.6203 - val_acc: 0.5923\n",
      "Epoch 25/30\n",
      "50000/50000 [==============================] - 24s 488us/step - loss: 0.0313 - acc: 0.9929 - val_loss: 2.7466 - val_acc: 0.5877\n",
      "Epoch 26/30\n",
      "50000/50000 [==============================] - 24s 488us/step - loss: 0.0325 - acc: 0.9929 - val_loss: 2.7062 - val_acc: 0.5946\n",
      "Epoch 27/30\n",
      "50000/50000 [==============================] - 24s 488us/step - loss: 0.0506 - acc: 0.9846 - val_loss: 2.8326 - val_acc: 0.5871\n",
      "Epoch 28/30\n",
      "50000/50000 [==============================] - 24s 489us/step - loss: 0.0727 - acc: 0.9768 - val_loss: 2.9067 - val_acc: 0.5718\n",
      "Epoch 29/30\n",
      "50000/50000 [==============================] - 24s 489us/step - loss: 0.0460 - acc: 0.9865 - val_loss: 2.9208 - val_acc: 0.5833\n",
      "Epoch 30/30\n",
      "50000/50000 [==============================] - 24s 489us/step - loss: 0.0295 - acc: 0.9920 - val_loss: 2.9328 - val_acc: 0.5949\n"
     ]
    }
   ],
   "source": [
    "K.clear_session()\n",
    "\n",
    "model = Sequential([\n",
    "    Conv2D(32, 3, activation='relu', input_shape=(32, 32, 3)),  # New layer\n",
    "    Flatten(),\n",
    "    Dense(256, activation='relu'),  # New layer\n",
    "    Dense(10, activation='softmax')\n",
    "    ])\n",
    "\n",
    "model.compile(optimizer=Adam(), loss=categorical_crossentropy, metrics=['acc'])\n",
    "\n",
    "history = model.fit(X_train, to_categorical(Y_train),\n",
    "                    validation_data=(X_val, to_categorical(Y_val)),\n",
    "                    batch_size=128, epochs=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Indeed, the Convolutional Neural Network (CNN) performed better than the Fully Connected one\n",
    "\n",
    "However it seems to overfit\n",
    "\n",
    "We can add some layers in order to deal with the overfitting problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 50000 samples, validate on 10000 samples\n",
      "Epoch 1/30\n",
      "50000/50000 [==============================] - 13s 253us/step - loss: 1.4689 - acc: 0.4792 - val_loss: 1.2908 - val_acc: 0.5458\n",
      "Epoch 2/30\n",
      "50000/50000 [==============================] - 12s 245us/step - loss: 1.1909 - acc: 0.5837 - val_loss: 1.1691 - val_acc: 0.5899\n",
      "Epoch 3/30\n",
      "50000/50000 [==============================] - 12s 246us/step - loss: 1.0791 - acc: 0.6233 - val_loss: 1.0740 - val_acc: 0.6251\n",
      "Epoch 4/30\n",
      "50000/50000 [==============================] - 13s 253us/step - loss: 0.9981 - acc: 0.6496 - val_loss: 1.0591 - val_acc: 0.6281\n",
      "Epoch 5/30\n",
      "50000/50000 [==============================] - 12s 247us/step - loss: 0.9191 - acc: 0.6793 - val_loss: 1.0351 - val_acc: 0.6345\n",
      "Epoch 6/30\n",
      "50000/50000 [==============================] - 12s 249us/step - loss: 0.8593 - acc: 0.7017 - val_loss: 0.9751 - val_acc: 0.6629\n",
      "Epoch 7/30\n",
      "50000/50000 [==============================] - 12s 248us/step - loss: 0.8003 - acc: 0.7208 - val_loss: 0.9972 - val_acc: 0.6535\n",
      "Epoch 8/30\n",
      "50000/50000 [==============================] - 12s 247us/step - loss: 0.7470 - acc: 0.7403 - val_loss: 0.9866 - val_acc: 0.6642\n",
      "Epoch 9/30\n",
      "50000/50000 [==============================] - 13s 256us/step - loss: 0.6939 - acc: 0.7582 - val_loss: 0.9711 - val_acc: 0.6747\n",
      "Epoch 10/30\n",
      "50000/50000 [==============================] - 13s 256us/step - loss: 0.6458 - acc: 0.7745 - val_loss: 0.9889 - val_acc: 0.6678\n",
      "Epoch 11/30\n",
      "50000/50000 [==============================] - 12s 247us/step - loss: 0.5970 - acc: 0.7933 - val_loss: 0.9813 - val_acc: 0.6786\n",
      "Epoch 12/30\n",
      "50000/50000 [==============================] - 12s 241us/step - loss: 0.5578 - acc: 0.8053 - val_loss: 0.9792 - val_acc: 0.6816\n",
      "Epoch 13/30\n",
      "50000/50000 [==============================] - 12s 243us/step - loss: 0.5158 - acc: 0.8194 - val_loss: 0.9823 - val_acc: 0.6758\n",
      "Epoch 14/30\n",
      "50000/50000 [==============================] - 12s 241us/step - loss: 0.4817 - acc: 0.8332 - val_loss: 1.0165 - val_acc: 0.6766\n",
      "Epoch 15/30\n",
      "50000/50000 [==============================] - 12s 243us/step - loss: 0.4459 - acc: 0.8452 - val_loss: 1.0420 - val_acc: 0.6739\n",
      "Epoch 16/30\n",
      "50000/50000 [==============================] - 12s 245us/step - loss: 0.4116 - acc: 0.8577 - val_loss: 1.1277 - val_acc: 0.6649\n",
      "Epoch 17/30\n",
      "50000/50000 [==============================] - 12s 246us/step - loss: 0.3848 - acc: 0.8674 - val_loss: 1.1140 - val_acc: 0.6675\n",
      "Epoch 18/30\n",
      "50000/50000 [==============================] - 12s 245us/step - loss: 0.3506 - acc: 0.8795 - val_loss: 1.1176 - val_acc: 0.6710\n",
      "Epoch 19/30\n",
      "50000/50000 [==============================] - 12s 244us/step - loss: 0.3276 - acc: 0.8878 - val_loss: 1.1297 - val_acc: 0.6751\n",
      "Epoch 20/30\n",
      "50000/50000 [==============================] - 12s 242us/step - loss: 0.3017 - acc: 0.8970 - val_loss: 1.1565 - val_acc: 0.6793\n",
      "Epoch 21/30\n",
      "50000/50000 [==============================] - 12s 250us/step - loss: 0.2705 - acc: 0.9075 - val_loss: 1.1876 - val_acc: 0.6734\n",
      "Epoch 22/30\n",
      "50000/50000 [==============================] - 12s 244us/step - loss: 0.2588 - acc: 0.9116 - val_loss: 1.2107 - val_acc: 0.6757\n",
      "Epoch 23/30\n",
      "50000/50000 [==============================] - 12s 242us/step - loss: 0.2387 - acc: 0.9181 - val_loss: 1.2323 - val_acc: 0.6758\n",
      "Epoch 24/30\n",
      "50000/50000 [==============================] - 12s 243us/step - loss: 0.2211 - acc: 0.9254 - val_loss: 1.2946 - val_acc: 0.6728\n",
      "Epoch 25/30\n",
      "50000/50000 [==============================] - 12s 243us/step - loss: 0.2053 - acc: 0.9295 - val_loss: 1.3046 - val_acc: 0.6720\n",
      "Epoch 26/30\n",
      "50000/50000 [==============================] - 12s 242us/step - loss: 0.1909 - acc: 0.9355 - val_loss: 1.3907 - val_acc: 0.6623\n",
      "Epoch 27/30\n",
      "50000/50000 [==============================] - 12s 243us/step - loss: 0.1844 - acc: 0.9375 - val_loss: 1.3911 - val_acc: 0.6628\n",
      "Epoch 28/30\n",
      "50000/50000 [==============================] - 12s 243us/step - loss: 0.1783 - acc: 0.9382 - val_loss: 1.4477 - val_acc: 0.6659\n",
      "Epoch 29/30\n",
      "50000/50000 [==============================] - 12s 242us/step - loss: 0.1682 - acc: 0.9428 - val_loss: 1.4527 - val_acc: 0.6667\n",
      "Epoch 30/30\n",
      "50000/50000 [==============================] - 12s 242us/step - loss: 0.1569 - acc: 0.9472 - val_loss: 1.4799 - val_acc: 0.6668\n"
     ]
    }
   ],
   "source": [
    "K.clear_session()\n",
    "\n",
    "model = Sequential([\n",
    "    Conv2D(32, 3, activation='relu', input_shape=(32, 32, 3)),\n",
    "    MaxPool2D(),  # New layer\n",
    "    Dropout(0.25),  # New layer\n",
    "  \n",
    "    Flatten(),\n",
    "    Dense(256, activation='relu'),\n",
    "    Dense(10, activation='softmax')\n",
    "    ])\n",
    "\n",
    "model.compile(optimizer=Adam(), loss=categorical_crossentropy, metrics=['acc'])\n",
    "\n",
    "history = model.fit(X_train, to_categorical(Y_train),\n",
    "                    validation_data=(X_val, to_categorical(Y_val)),\n",
    "                    batch_size=128, epochs=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "And finally let's see what will be the performance if we add more layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 50000 samples, validate on 10000 samples\n",
      "Epoch 1/30\n",
      "50000/50000 [==============================] - 33s 662us/step - loss: 1.9838 - acc: 0.2715 - val_loss: 1.7970 - val_acc: 0.3436\n",
      "Epoch 2/30\n",
      "50000/50000 [==============================] - 32s 633us/step - loss: 1.6924 - acc: 0.3879 - val_loss: 1.5745 - val_acc: 0.4325\n",
      "Epoch 3/30\n",
      "50000/50000 [==============================] - 32s 631us/step - loss: 1.5687 - acc: 0.4285 - val_loss: 1.5144 - val_acc: 0.4541\n",
      "Epoch 4/30\n",
      "50000/50000 [==============================] - 32s 631us/step - loss: 1.4784 - acc: 0.4649 - val_loss: 1.3861 - val_acc: 0.4955\n",
      "Epoch 5/30\n",
      "50000/50000 [==============================] - 32s 632us/step - loss: 1.4044 - acc: 0.4917 - val_loss: 1.2999 - val_acc: 0.5352\n",
      "Epoch 6/30\n",
      "50000/50000 [==============================] - 32s 633us/step - loss: 1.3376 - acc: 0.5169 - val_loss: 1.2368 - val_acc: 0.5606\n",
      "Epoch 7/30\n",
      "50000/50000 [==============================] - 32s 634us/step - loss: 1.2811 - acc: 0.5403 - val_loss: 1.1940 - val_acc: 0.5743\n",
      "Epoch 8/30\n",
      "50000/50000 [==============================] - 32s 633us/step - loss: 1.2280 - acc: 0.5640 - val_loss: 1.1520 - val_acc: 0.5899\n",
      "Epoch 9/30\n",
      "50000/50000 [==============================] - 32s 633us/step - loss: 1.1849 - acc: 0.5767 - val_loss: 1.1036 - val_acc: 0.6074\n",
      "Epoch 10/30\n",
      "50000/50000 [==============================] - 32s 632us/step - loss: 1.1414 - acc: 0.5932 - val_loss: 1.0995 - val_acc: 0.6099\n",
      "Epoch 11/30\n",
      "50000/50000 [==============================] - 32s 633us/step - loss: 1.1042 - acc: 0.6078 - val_loss: 1.0674 - val_acc: 0.6248\n",
      "Epoch 12/30\n",
      "50000/50000 [==============================] - 32s 633us/step - loss: 1.0643 - acc: 0.6248 - val_loss: 1.0045 - val_acc: 0.6448\n",
      "Epoch 13/30\n",
      "50000/50000 [==============================] - 32s 632us/step - loss: 1.0356 - acc: 0.6343 - val_loss: 0.9768 - val_acc: 0.6536\n",
      "Epoch 14/30\n",
      "50000/50000 [==============================] - 32s 650us/step - loss: 1.0035 - acc: 0.6453 - val_loss: 0.9491 - val_acc: 0.6671\n",
      "Epoch 15/30\n",
      "50000/50000 [==============================] - 32s 634us/step - loss: 0.9742 - acc: 0.6565 - val_loss: 0.9435 - val_acc: 0.6683\n",
      "Epoch 16/30\n",
      "50000/50000 [==============================] - 32s 632us/step - loss: 0.9459 - acc: 0.6664 - val_loss: 0.9260 - val_acc: 0.6792\n",
      "Epoch 17/30\n",
      "50000/50000 [==============================] - 32s 633us/step - loss: 0.9198 - acc: 0.6748 - val_loss: 0.9302 - val_acc: 0.6743\n",
      "Epoch 18/30\n",
      "50000/50000 [==============================] - 32s 634us/step - loss: 0.8923 - acc: 0.6871 - val_loss: 0.9187 - val_acc: 0.6781\n",
      "Epoch 19/30\n",
      "50000/50000 [==============================] - 32s 634us/step - loss: 0.8703 - acc: 0.6945 - val_loss: 0.8407 - val_acc: 0.7054\n",
      "Epoch 20/30\n",
      "50000/50000 [==============================] - 32s 632us/step - loss: 0.8478 - acc: 0.7029 - val_loss: 0.8456 - val_acc: 0.7007\n",
      "Epoch 21/30\n",
      "50000/50000 [==============================] - 32s 633us/step - loss: 0.8296 - acc: 0.7088 - val_loss: 0.8185 - val_acc: 0.7188\n",
      "Epoch 22/30\n",
      "50000/50000 [==============================] - 32s 634us/step - loss: 0.8074 - acc: 0.7178 - val_loss: 0.7947 - val_acc: 0.7261\n",
      "Epoch 23/30\n",
      "50000/50000 [==============================] - 32s 633us/step - loss: 0.7837 - acc: 0.7270 - val_loss: 0.7917 - val_acc: 0.7268\n",
      "Epoch 24/30\n",
      "50000/50000 [==============================] - 32s 633us/step - loss: 0.7676 - acc: 0.7316 - val_loss: 0.7921 - val_acc: 0.7242\n",
      "Epoch 25/30\n",
      "50000/50000 [==============================] - 32s 632us/step - loss: 0.7465 - acc: 0.7383 - val_loss: 0.7642 - val_acc: 0.7341\n",
      "Epoch 26/30\n",
      "50000/50000 [==============================] - 32s 632us/step - loss: 0.7326 - acc: 0.7426 - val_loss: 0.7569 - val_acc: 0.7385\n",
      "Epoch 27/30\n",
      "50000/50000 [==============================] - 32s 632us/step - loss: 0.7105 - acc: 0.7518 - val_loss: 0.7537 - val_acc: 0.7375\n",
      "Epoch 28/30\n",
      "50000/50000 [==============================] - 32s 632us/step - loss: 0.6904 - acc: 0.7597 - val_loss: 0.7303 - val_acc: 0.7460\n",
      "Epoch 29/30\n",
      "50000/50000 [==============================] - 32s 632us/step - loss: 0.6811 - acc: 0.7622 - val_loss: 0.7275 - val_acc: 0.7459\n",
      "Epoch 30/30\n",
      "50000/50000 [==============================] - 32s 632us/step - loss: 0.6678 - acc: 0.7656 - val_loss: 0.7309 - val_acc: 0.7478\n"
     ]
    }
   ],
   "source": [
    "K.clear_session()\n",
    "\n",
    "model = Sequential([\n",
    "    Conv2D(32, 3, padding='same', activation='relu', input_shape=(32, 32, 3)),\n",
    "    Conv2D(32, 3, padding='same', activation='relu'),  # New layer\n",
    "  \n",
    "    MaxPool2D(),\n",
    "    Dropout(0.25),\n",
    "  \n",
    "    Conv2D(64, 3, padding='same', activation='relu'),  # New layer\n",
    "    Conv2D(64, 3, padding='same', activation='relu'),  # New layer\n",
    "  \n",
    "    MaxPool2D(),  # New layer\n",
    "    Dropout(0.25),  # New layer\n",
    "  \n",
    "    Conv2D(128, 3, padding='same', activation='relu'),  # New layer\n",
    "    Conv2D(128, 3, padding='same', activation='relu'),  # New layer\n",
    "  \n",
    "    MaxPool2D(),  # New layer\n",
    "    Dropout(0.25),  # New layer\n",
    "  \n",
    "    Flatten(),\n",
    "    Dense(512, activation='relu'),\n",
    "    Dropout(0.25),\n",
    "  \n",
    "    Dense(10, activation='softmax')])\n",
    "\n",
    "model.compile(optimizer=rmsprop(lr=0.0001, decay=1e-6), loss=categorical_crossentropy, metrics=['acc'])\n",
    "\n",
    "history = model.fit(X_train, to_categorical(Y_train),\n",
    "                    validation_data=(X_val, to_categorical(Y_val)),\n",
    "                    batch_size=128, epochs=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "This means that:\n",
    "- the **complexity** of the patterns accuaries more sofisticated approaches\n",
    "- the **size** of the model is important\n",
    "- the **type** of the layers is important\n",
    "- the **architecture** of the network is important\n",
    "- there are ways to get **better results** for the same task on the same data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## The end\n",
    "### of the data and model experimentation"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  },
  "rise": {
   "autolaunch": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
